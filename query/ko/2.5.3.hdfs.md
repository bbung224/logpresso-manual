### 2.5.3. 하둡 커넥터

#### 2.5.3.1. HDFS 파일 목록 조회

HDFS 사이트 설정이 완료된 후, 아래의 쿼리 커맨드를 사용하여 HDFS 파일시스템을 탐색하거나 파일을 조회할 수 있습니다.

~~~~
	hdfs 사이트이름 ls 절대경로
	hdfs 사이트이름 lsr 절대경로
~~~~

예를 들어, 하둡을 설치한 직후 사이트 이름으로 vm을 설정했다면 아래와 같은 쿼리 결과를 확인할 수 있습니다.

~~~~
    araqne@logdb> query hdfs vm ls /
    {block_size=0, file_size=0, group=supergroup, modified_at=Sun Sep 15 15:46:32 
    KST 2013, name=tmp, owner=root, path=/tmp, permission=rwxr-xrwx, replication=0, 
    type=dir}
    total 1 rows, elapsed 5.1s
~~~~

ls 조회 결과 필드들은 각각 아래의 의미를 가집니다:

 * type (문자열): 디렉터리인 경우 dir, 파일인 경우 file
 * name (문자열): 파일 이름
 * path (문자열): 파일의 절대 경로
 * replication (정수): 복제본 수, 디렉터리인 경우 0
 * file\_size (정수): 파일 크기, 디렉터리인 경우 0
 * block\_size (정수): 블럭 크기, 디렉터리인 경우 0
 * modified\_at (날짜): 마지막 수정 시각
 * permission (문자열): 권한 설정
 * owner (문자열): 소유자
 * group (문자열): 소유그룹
 
ls 대신 lsr 옵션을 사용하는 경우 지정된 디렉터리 뿐 아니라 하위의 모든 디렉터리를 탐색합니다.

#### 2.5.3.2. HDFS 파일 쿼리

HDFS 사이트 설정이 완료된 후, 아래의 쿼리 커맨드를 사용하여 HDFS 파일시스템을 탐색하거나 파일을 조회할 수 있습니다.

1) 텍스트 파일 조회

HDFS의 텍스트 파일을 줄 단위로 읽어옵니다.

문법

~~~~
	hdfs 사이트이름 cat [offset=건너뛸 갯수] [limit=최대 갯수] 파일경로
~~~~

 * 건너뛸 갯수: 건너뛸 행 갯수를 지정합니다. 기본값은 0입니다.
 * 최대 갯수: 가져올 최대 행 갯수를 지정합니다. 기본값은 무제한입니다.
 * 파일 경로: HDFS 파일 절대 경로를 입력합니다.

읽어온 행은 아래와 같이 line 필드로 조회됩니다.

예시) /tmp/LICENSE.txt 파일의 첫 행은 건너뛰고 5개 행을 조회

~~~~
    araqne@logdb> query hdfs vm cat offset=1 limit=5 /tmp/LICENSE.txt
    {line=                                 Apache License}
    {line=                           Version 2.0, January 2004}
    {line=                        http://www.apache.org/licenses/}
    {line=}
    {line=   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION}
    total 5 rows, elapsed 0.3s
~~~~

2) 시퀀스 파일 조회

HDFS의 시퀀스 파일을 레코드 단위로 읽어옵니다. 압축 방식 (레코드 단위 압축, 블럭 단위 압축)에 관계없이 조회 가능하며, 하둡에 내장된 아래와 같은 Writable 구현들을 자바 프리미티브로 자동 변환하여 조회합니다:

 * Text: 문자열 타입으로 변환됩니다.
 * NullWritable: 널 타입으로 변환됩니다.
 * IntWritable: 4바이트 정수 타입으로 변환됩니다.
 * LongWritable: 8바이트 정수 타입으로 변환됩니다.
 * DoubleWritable: 배정도 실수 타입으로 변환됩니다.
 * FloatWritable: 단정도 실수 타입으로 변환됩니다.
 * BooleanWritable: 불린 타입으로 변환됩니다.
 * VIntWritable: 4바이트 정수 타입으로 변환됩니다.
 * VLongWritable: 8바이트 정수 타입으로 변환됩니다.
 * MapWritable: 키는 원본 타입에 관계없이 문자열로 변환되며, 값은 위에 언급된 규칙대로 변환됩니다.

문법

~~~~
	hdfs 사이트이름 cat format=sequence [offset=건너뛸 갯수] [limit=최대 갯수] 파일경로
~~~~
 
 * 건너뛸 갯수: 건너뛸 행 갯수를 지정합니다. 기본값은 0입니다.
 * 최대 갯수: 가져올 최대 행 갯수를 지정합니다. 기본값은 무제한입니다.
 * 파일 경로: HDFS 파일 절대 경로를 입력합니다.

키 필드 이름은 key로 지정되고, 값 필드 이름은 값 타입이 MapWritable이 아닌 경우 value로 지정됩니다. 값 타입이 MapWritable 인 경우에는 내부의 키/값 매핑이 반환되는 행의 필드로 반환됩니다.

예시) JMX 클래스로딩 모니터링 로그를 HDFS에서 시퀀스 파일로 적재 후 조회

~~~~
    araqne@logdb> query hdfs vm cat format=sequence limit=2 /tmp/classloading.seq
    {LoadedClassCount=9100, ObjectName=java.lang:type=ClassLoading, TotalLoad
      edClassCount=9100, UnloadedClassCount=0, Verbose=false, _id=85, _table=
      classloading, _time=Sun Sep 15 14:29:13 KST 2013, key=1}
    {LoadedClassCount=9100, ObjectName=java.lang:type=ClassLoading, TotalLoad
      edClassCount=9100, UnloadedClassCount=0, Verbose=false, _id=84, _table=
      classloading, _time=Sun Sep 15 14:29:08 KST 2013, key=2}
    total 2 rows, elapsed 0.1s
~~~~

3) JSON 파일 조회

HDFS의 JSON 파일을 줄 단위로 읽어서 해석합니다.

문법

~~~~
	hdfs 사이트이름 cat format=json [offset=건너뛸 갯수] [limit=최대 갯수] 파일경로
~~~~

 * 건너뛸 갯수: 건너뛸 행 갯수를 지정합니다. 기본값은 0입니다.
 * 최대 갯수: 가져올 최대 행 갯수를 지정합니다. 기본값은 무제한입니다.
 * 파일 경로: HDFS 파일 절대 경로를 입력합니다.

예시) HDFS의 iis.json 파일을 조회

~~~~
    araqne@logdb> query hdfs vm cat format=json limit=1 /tmp/iis.json
    {_id=68, _table=iis, _time=2013-09-10 19:52:37+0900, line=#Fields: date time s-sitename 
      s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) 
      sc-status sc-substatus sc-win32-status}
    total 1 rows, elapsed 0.1s
~~~~

동일한 파일을 JSON 포맷으로 지정하지 않고 텍스트로 읽으면 아래와 같이 JSON 문법으로 된 원문을 확인할 수 있습니다.

~~~~
    araqne@logdb> query hdfs vm cat limit=1 /tmp/iis.json
    {line={"_id":68,"_table":"iis","line":"#Fields: date time s-sitename s-ip cs-method 
      cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) sc-status sc-substatus 
      sc-win32-status","_time":"2013-09-10 19:52:37+0900"}}
    total 1 rows, elapsed 0.1s
~~~~

4) CSV 파일 조회

HDFS의 CSV 파일을 조회합니다. 첫 줄은 열 제목으로 인식합니다. 데이터에 개행 문자가 있더라도 CSV 규칙에 맞으면 여러 개의 줄을 하나의 셀 데이터로 인식합니다.

문법

~~~~
	hdfs 사이트이름 cat format=csv [offset=건너뛸 갯수] [limit=최대 갯수] 파일경로
~~~~

 * 건너뛸 갯수: 건너뛸 행 갯수를 지정합니다. 기본값은 0입니다.
 * 최대 갯수: 가져올 최대 행 갯수를 지정합니다. 기본값은 무제한입니다.
 * 파일 경로: HDFS 파일 절대 경로를 입력합니다.

예시) Reddit Malware 클릭 통계 CSV 조회

~~~~
    araqne@logdb> query hdfs vm cat format=csv limit=3 /tmp/malware.csv 
		| fields domain, title,  ups, downs
    {domain=blog.virustotal.com, downs=6, 
	 title=VirusTotal adds PCAP support with file extraction & 
	 Snort/Suricata alerts, ups=40}
    {domain=self.Malware, downs=11, 
	 title=Where To Start With Malware Analysis, ups=43}
    {domain=deependresearch.org, downs=1, 
	 title=DeepEnd Research releases Library of Malware Traffic 
     Patterns (good IDS alert xref), ups=33}
    total 3 rows, elapsed 0.1s
~~~~

#### 2.5.3.3. HDFS 파일 출력

HDFS 사이트 설정이 완료된 후, 아래의 쿼리 커맨드를 사용하여 로그프레소 쿼리 결과를 HDFS 파일로 출력할 수 있습니다. 만약 이미 지정된 HDFS 경로에 파일이 존재한다면 쿼리 생성이 실패합니다.

1) HDFS 텍스트 파일 출력

지정된 HDFS 경로에 쿼리 결과를 텍스트 파일로 출력합니다.

~~~~
	hdfs 사이트이름 put [fields=쉼표로 구분된 필드 이름] 파일경로
~~~~

fields 옵션 지정을 생략하면 기본값으로 line 필드를 출력합니다. 만약 출력하려는 필드 값이 존재하지 않으면 -를 출력합니다. 여러 개의 필드를 출력할 때는 탭으로 구분합니다.

예시) JMX 클래스로딩 로그 중 UnloadedClassCount와 LoadedClassCount만 /tmp/classloading.txt 경로에 출력

~~~~
	table classloading | hdfs vm put 
		fields=UnloadedClassCount,LoadedClassCount /tmp/classloading.txt
~~~~

HDFS 출력 파일은 아래와 같이 저장됩니다.

~~~~
    0	9100
    0	9100
    0	9089
    0	9089
    0	8997
~~~~

2) HDFS 시퀀스 파일 출력

지정된 HDFS 경로에 쿼리 결과를 시퀀스 파일로 출력합니다.

~~~~
	hdfs 사이트이름 put format=sequence [fields=쉼표로 구분된 필드 이름] 
		[key_type=키 타입] [key_field=키 필드 이름] [value_type=값 타입] 
		[value_field=값 필드 이름] [compression_type=압축 유형] 파일경로
~~~~

각 옵션은 아래와 같습니다:

 * fields: 출력 대상 필드 이름들을 지정합니다. 미설정 시 전체 필드가 출력됩니다.
 * key_type: 키 타입을 지정합니다.
 * key_field: 키 필드 이름을 지정합니다. 미설정 시 1부터 시작하는 LongWritable 카운터가 기록됩니다.
 * value_type: 값 타입을 지정합니다.
 * value_field: 값 필드 이름을 지정합니다. 미설정 시 전체 필드가 하나의 MapWritable로 묶여서 출력됩니다.
 * compression_type: record이면 레코드 단위 압축, block이면 블럭 단위 압축, 미설정시 압축하지 않습니다.

타입은 아래의 목록 중 하나를 지정할 수 있습니다:

 * string: 문자열, 하둡의 Text 타입과 매핑됩니다.
 * bool: 불린, 하둡의 BooleanWritable 타입과 매핑됩니다.
 * int:32비트 정수, 하둡의 IntWritable 타입과 매핑됩니다.
 * long: 64비트 정수, 하둡의 LongWritable 타입과 매핑됩니다.
 * float: 단정도 실수, 하둡의 FloatWritable 타입과 매핑됩니다.
 * double: 배정도 실수, 하둡의 DoubleWritable 타입과 매핑됩니다.

키나 값 중 하나라도 비어있으면 그 행은 출력하지 않습니다. 만약 값은 존재하지만 타입이 불일치한다면, string인 경우는 자동으로 문자열로 변환하고, int/long/float/double의 경우 0으로, bool은 false로 출력됩니다. 정밀도를 손상하지 않고 변환할 수 있는 경우에는 변환 후 출력합니다. 가령 long 타입으로 지정된 상태에서 int 값이 들어오면 long으로 변환됩니다.

예시 1) JMX 클래스로딩 로그 전체를 시퀀스 파일로 출력

~~~~
	table classloading | hdfs vm put format=sequence /tmp/classloading.seq
~~~~

예시 2) JMX 클래스로딩 로그 중 LoadedClassCount 값을 출력

~~~~
	table classloading | hdfs vm put format=sequence value_type=long 
		value_field=LoadedClassCount /tmp/classloading.seq
~~~~

3) HDFS JSON 파일 출력

지정된 HDFS 파일 경로에 쿼리 결과를 JSON 파일로 출력합니다.

~~~~
	hdfs 사이트이름 put format=json [fields=쉼표로 구분된 필드 이름] 파일경로
~~~~

fields 옵션을 지정하면 지정된 필드들만 JSON으로 출력됩니다. fields 옵션을 생략하면 모든 필드값이 JSON으로 출력됩니다.

예시 1) JMX 클래스로딩 로그를 JSON 파일로 출력

~~~~
	table classloading | hdfs vm put format=json /tmp/classloading.json
~~~~

HDFS 출력 파일은 아래와 같이 저장됩니다.

~~~~
	araqne@logdb> query hdfs vm cat limit=1 /tmp/classloading.json
	{line={"Verbose":false,"LoadedClassCount":9100,"_id":85,
	"UnloadedClassCount":0,"_table":"classloading",
	"ObjectName":"java.lang:type=ClassLoading",
	"_time":"2013-09-15 14:29:13+0900","TotalLoadedClassCount":9100}}
	total 1 rows, elapsed 0.1s
~~~~

예시 2) JMX 클래스로딩 로그 중 LoadedClassCount, UnloadedClassCount, TotalLoadedClassCount 출력

~~~~
	table classloading | hdfs vm put format=json 
		fields=LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount 
		/tmp/classloading.json
~~~~

HDFS 출력 파일은 아래와 같이 저장됩니다.

~~~~
	araqne@logdb> query hdfs vm cat limit=1 /tmp/classloading.json
	{line={"TotalLoadedClassCount":9100,"LoadedClassCount":9100,"UnloadedClassCount":0}}
	total 1 rows, elapsed 0.1s
~~~~

4) HDFS CSV 파일 출력

지정된 HDFS 파일 경로에 쿼리 결과를 CSV 파일로 출력합니다.

~~~~
	hdfs 사이트이름 put format=csv [fields=쉼표로 구분된 필드 이름] 파일경로
~~~~

CSV 파일 생성 시 첫 줄에 필드 이름 목록이 출력됩니다. 지정된 필드 값이 존재하지 않는 경우 빈 문자열로 표시됩니다. fields 옵션을 생략하면 line 필드가 출력됩니다.

예시) JMX 클래스로딩 로그 중 LoadedClassCount, UnloadedClassCount, TotalLoadedClassCount 출력

~~~~
	table classloading | hdfs vm put format=csv 
		fields=LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount 
		/tmp/classloading.csv
~~~~

HDFS 출력 파일은 아래와 같이 저장됩니다.

~~~~
    araqne@logdb> query hdfs vm cat limit=2 /tmp/classloading.csv
    {line="LoadedClassCount","UnloadedClassCount","TotalLoadedClassCount"}
    {line="9100","0","9100"}
    total 2 rows, elapsed 0.1s
~~~~
